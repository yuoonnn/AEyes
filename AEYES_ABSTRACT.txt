AEyes is a wearable assistive system designed to enhance mobility, safety, and independence for individuals with visual impairment through cloud-assisted multimodal artificial intelligence and image processing. Addressing critical challenges such as limited real-time hazard awareness and the prohibitive cost of existing solutions, AEyes integrates a camera-enabled smart-glasses prototype with real-time scene understanding using the OpenAI GPT-4 Vision API, optical character recognition for text reading, text-to-speech for natural language audio feedback, currency denomination identification, and optional face recognition capabilities. The system delivers context-aware, descriptive guidance through bone-conduction earphones or standard earphones to maintain ambient sound awareness, supporting voice-activated interaction and tactile control buttons for critical functions. The solution leverages OpenAI's vision-language model accessed via a connected smartphone or network connection, with a companion mobile application enabling initial setup, accessibility customization including volume and language preferences and dark mode support, and device management over Bluetooth. GPS-based location callouts and caregiver-oriented SMS alerting for emergencies or live location updates enhance user safety and caregiver awareness, while the hardware is designed to be ergonomic, lightweight, and water-resistant with strategically positioned electronics for optimal balance and comfort. Evaluation findings demonstrate significant improvements in obstacle detection accuracy, enhanced user confidence in independent navigation, reduced response times for emergency situations, and positive user feedback regarding the natural language interaction and real-time assistance capabilities, validating the system's effectiveness as a cost-effective alternative to existing high-end assistive technologies.

